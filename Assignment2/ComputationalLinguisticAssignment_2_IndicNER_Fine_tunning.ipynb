{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6019965",
   "metadata": {},
   "source": [
    "### Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d74b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\n",
    "import json\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda06593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9437c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a0f1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Selecting the device\n",
    "device = torch.device(\"cuda:1\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850063b0",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cfb142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForTokenClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([9, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([9]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai4bharat/indicNER\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39mnum_labels)\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indicNER\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mai4bharat/indicNER\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39mnum_labels )\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.12/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3503\u001b[0m         model,\n\u001b[1;32m   3504\u001b[0m         state_dict,\n\u001b[1;32m   3505\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3506\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3507\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3508\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3509\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3510\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3511\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3512\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3513\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3514\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3515\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3516\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   3517\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3518\u001b[0m     )\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.12/site-packages/transformers/modeling_utils.py:3977\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   3974\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3975\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3976\u001b[0m         )\n\u001b[0;32m-> 3977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3980\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForTokenClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([7, 768]) from checkpoint, the shape in current model is torch.Size([9, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([7]) from checkpoint, the shape in current model is torch.Size([9]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "# IndicNER\n",
    "num_labels = 9\n",
    "text_name = \"words\"\n",
    "label_name = \"ner\"\n",
    "\n",
    "config = AutoConfig.from_pretrained('ai4bharat/indicNER', num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indicNER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indicNER', num_labels=num_labels )\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa2b0d5",
   "metadata": {},
   "source": [
    "The above code explains that we cannot modify the number of labels for IndicNER, as it is already fine-tuned with NER task, and does not add any additional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f1fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndicNER\n",
    "num_labels = 7\n",
    "text_name = \"words\"\n",
    "label_name = \"ner\"\n",
    "\n",
    "config = AutoConfig.from_pretrained('ai4bharat/indicNER')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indicNER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indicNER' )\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd591d",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450e5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id_dict = {\n",
    "    'O' : 0,\n",
    "    'B-PER' : 1,\n",
    "    'I-PER' : 2,\n",
    "    'B-ORG' : 3,\n",
    "    'I-ORG' : 4,\n",
    "    'B-LOC' : 5,\n",
    "    'I-LOC' : 6,\n",
    "    'B-MISC': 7, # Adding the extra two label will help to use the compute metric of the trainer\n",
    "    'I-MISC' : 8 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864b515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_2_label = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdf0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset( file_path, text_name = text_name, label_name = label_name, dataset_type = None, \n",
    "                 num_samples_to_take = None ):\n",
    "    \n",
    "    data_dir = { text_name : [ ], label_name : [ ] }\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data_dir[text_name].append(json.loads(line)[text_name])\n",
    "            data_dir[label_name].append( [ label_to_id_dict[i] for i in ( json.loads(line)[label_name] ) ] )\n",
    "    if dataset_type == 'train':\n",
    "        # For sampling the initial 20000 samples\n",
    "        data_dir[text_name] = data_dir[text_name][0:num_samples_to_take]\n",
    "        data_dir[label_name] = data_dir[label_name][ 0:num_samples_to_take]\n",
    "    data = Dataset.from_dict(data_dir)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ad1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_dataset(\"./bn_IndicNER_v1.0/bn_train.json\", dataset_type = \"train\", num_samples_to_take=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ecb002",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = make_dataset(\"./bn_IndicNER_v1.0/bn_val.json\", num_samples_to_take=100)\n",
    "test_data = make_dataset(\"./bn_IndicNER_v1.0/bn_test.json\", num_samples_to_take=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b078ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'ner'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577fab30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'ner'],\n",
       "    num_rows: 4859\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7701999",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\"\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the text and match the target label with the text token\n",
    "    Input: A batch of sentence\n",
    "        A dict:\n",
    "            'words': A list of sentences denoted as a list of words\n",
    "                        Desired Input to the model\n",
    "            'ner': A list of desired labels\n",
    "    Output: Tokenized Sentence\n",
    "        A dict: dict:\n",
    "            'words': A list of sentences denoted as a list of words\n",
    "                        Desired Input to the model\n",
    "            'ner': A list of desired labels\n",
    "            'input_ids': Id of the tokenized words\n",
    "                            A list of sentences denoted as \n",
    "                            a list of Id s of the consitituted word tokens\n",
    "                            # Input to the Transformer model\n",
    "            'attention_masks': A binary list\n",
    "                                1 indicates the text token, model will attend to\n",
    "                                0 indicates the padding token and special token, the model will not attend to.\n",
    "            'labels': A list of numbers\n",
    "                        Denting the target ground truth label\n",
    "                        generated from the 'ner' field of Input\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_name]):\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc0ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3432.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c833e947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'ner', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the toenized data\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9544669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on validation dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4859/4859 [00:01<00:00, 3517.49 examples/s]\n",
      "Running tokenizer on test dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 607/607 [00:00<00:00, 3602.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = val_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on test dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fafecd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27872efd",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b724219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1876914/2858202782.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id_2_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_2_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58557501",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cfdec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(batch_size, lr , model, train_data = train_data, val_data = val_data,\n",
    "                  tokenizer = tokenizer, data_collator = data_collator, compute_metrics = compute_metrics):\n",
    "                    \n",
    "    \n",
    "    \n",
    "    # Setting the TrainingArguments\n",
    "    args=TrainingArguments(\n",
    "        output_dir='output_dir',\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate = lr,\n",
    "        num_train_epochs = 3 # As asked in the question\n",
    "        )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        args=args,\n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    eval_metric = trainer.evaluate(val_data)\n",
    "    \n",
    "    return (trainer, train_result, eval_metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1902ce",
   "metadata": {},
   "source": [
    "#### Hyper-parameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee930dd",
   "metadata": {},
   "source": [
    "We have tuned the batch size and learning-rate as two are the most important hyper-parameters. \n",
    "Batch Size: Larger Batch size might result in a smoother convergence during Gradient descent, with a huge cost of calculating the gradients. Whereas smaller batch size might result ina more haphazard convergence but each step of Gradient Descent is fast. Therefore a we need to fine tune the optimum.\n",
    "\n",
    "Learning Rate: A Higher Learning Rate indicates larger step, which might lead the model to go in bad optima, and lower earning rate might not help the model to converge.\n",
    "\n",
    "We use a random sapling approach and done only 3 experiments due to limited resources for Hyper-parameter search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15ca8703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_search():\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indicNER' )\n",
    "    model=model.to(device)\n",
    "    \n",
    "    tunable_hyper_param = {'batch_size' : [8, 16, 32],\n",
    "                      'lr': [1e-2, 1e-3, 1e-4, 1e-5]}\n",
    "    batch_size = tunable_hyper_param['batch_size'][random.randint(0,2)] \n",
    "                # Generating a random number between 0 and 2 (both included)\n",
    "    lr = tunable_hyper_param['lr'][random.randint(0,3)]\n",
    "                # Generating a random number between 0 and 3 (both included)\n",
    "    \n",
    "    print('Batch size Learning Rate')\n",
    "    print(batch_size, lr)\n",
    "    \n",
    "    (trainer, train_result, eval_metric) = training_model(batch_size, lr, model)\n",
    "    \n",
    "    return  (trainer, train_result, eval_metric, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9195a",
   "metadata": {},
   "source": [
    "##### Hyper-parameter search Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f93c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size Learning Rate\n",
      "32 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 07:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "(trainer, train_result, eval_metric, batch_size, lr) = hyper_param_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2caf6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch Size = 32, and Learning Rate = 0.01 :\n",
      "The Evaluation Metric on the validation Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7981597781181335,\n",
       " 'eval_LOC_precision': 0.0,\n",
       " 'eval_LOC_recall': 0.0,\n",
       " 'eval_LOC_f1': 0.0,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.0,\n",
       " 'eval_ORG_recall': 0.0,\n",
       " 'eval_ORG_f1': 0.0,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.0,\n",
       " 'eval_PER_recall': 0.0,\n",
       " 'eval_PER_f1': 0.0,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.0,\n",
       " 'eval_overall_recall': 0.0,\n",
       " 'eval_overall_f1': 0.0,\n",
       " 'eval_overall_accuracy': 0.8187198114551364,\n",
       " 'eval_runtime': 24.6876,\n",
       " 'eval_samples_per_second': 196.82,\n",
       " 'eval_steps_per_second': 6.157,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('For Batch Size = {}, and Learning Rate = {} :'.format(batch_size, lr))\n",
    "print('The Evaluation Metric on the validation Data: ')\n",
    "eval_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879206d",
   "metadata": {},
   "source": [
    "##### Hyper-parameter search experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0c47a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size Learning Rate\n",
      "32 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 07:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(trainer_2, train_result_2, eval_metric_2, batch_size_2, lr_2) = hyper_param_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "333995b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch Size = 32, and Learning Rate = 0.01 :\n",
      "The Evaluation Metric on the validation Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7981597781181335,\n",
       " 'eval_LOC_precision': 0.0,\n",
       " 'eval_LOC_recall': 0.0,\n",
       " 'eval_LOC_f1': 0.0,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.0,\n",
       " 'eval_ORG_recall': 0.0,\n",
       " 'eval_ORG_f1': 0.0,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.0,\n",
       " 'eval_PER_recall': 0.0,\n",
       " 'eval_PER_f1': 0.0,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.0,\n",
       " 'eval_overall_recall': 0.0,\n",
       " 'eval_overall_f1': 0.0,\n",
       " 'eval_overall_accuracy': 0.8187198114551364,\n",
       " 'eval_runtime': 24.9677,\n",
       " 'eval_samples_per_second': 194.611,\n",
       " 'eval_steps_per_second': 6.088,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('For Batch Size = {}, and Learning Rate = {} :'.format(batch_size_2, lr_2))\n",
    "print('The Evaluation Metric on the validation Data: ')\n",
    "eval_metric_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ee78a",
   "metadata": {},
   "source": [
    "##### Hyper-parameter search Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45c49fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size Learning Rate\n",
      "32 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 07:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "(trainer_3, train_result_3, eval_metric_3, batch_size_3, lr_3) = hyper_param_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d4c7114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch Size = 32, and Learning Rate = 0.01 :\n",
      "The Evaluation Metric on the validation Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7981597781181335,\n",
       " 'eval_LOC_precision': 0.0,\n",
       " 'eval_LOC_recall': 0.0,\n",
       " 'eval_LOC_f1': 0.0,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.0,\n",
       " 'eval_ORG_recall': 0.0,\n",
       " 'eval_ORG_f1': 0.0,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.0,\n",
       " 'eval_PER_recall': 0.0,\n",
       " 'eval_PER_f1': 0.0,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.0,\n",
       " 'eval_overall_recall': 0.0,\n",
       " 'eval_overall_f1': 0.0,\n",
       " 'eval_overall_accuracy': 0.8187198114551364,\n",
       " 'eval_runtime': 25.1842,\n",
       " 'eval_samples_per_second': 192.939,\n",
       " 'eval_steps_per_second': 6.036,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('For Batch Size = {}, and Learning Rate = {} :'.format(batch_size_3, lr_3))\n",
    "print('The Evaluation Metric on the validation Data: ')\n",
    "eval_metric_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d80aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_setting():\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indicNER' )\n",
    "    model=model.to(device)\n",
    "    \n",
    "    batch_size = 8\n",
    "    lr = 1e-5\n",
    "    print('Batch size Learning Rate')\n",
    "    print(batch_size, lr)\n",
    "    \n",
    "    (trainer, train_result, eval_metric) = training_model(batch_size, lr, model)\n",
    "    \n",
    "    return  (trainer, train_result, eval_metric, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be09c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size Learning Rate\n",
      "8 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 11:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.156400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory output_dir/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory output_dir/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory output_dir/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 13:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(trainer_4, train_result_4, eval_metric_4, batch_size_4, lr_4) = hyper_param_setting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fdc8ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch Size = 8, and Learning Rate = 1e-05 :\n",
      "The Evaluation Metric on the validation Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19311270117759705,\n",
       " 'eval_LOC_precision': 0.7546480743691899,\n",
       " 'eval_LOC_recall': 0.8086090359302739,\n",
       " 'eval_LOC_f1': 0.7806972351021809,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.6146688560481663,\n",
       " 'eval_ORG_recall': 0.6413478012564249,\n",
       " 'eval_ORG_f1': 0.6277249860257126,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.8009319181982915,\n",
       " 'eval_PER_recall': 0.8366684694429422,\n",
       " 'eval_PER_f1': 0.8184102631926993,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.745805561939784,\n",
       " 'eval_overall_recall': 0.7857142857142857,\n",
       " 'eval_overall_f1': 0.7652399481193256,\n",
       " 'eval_overall_accuracy': 0.9458969478005258,\n",
       " 'eval_runtime': 25.2989,\n",
       " 'eval_samples_per_second': 192.063,\n",
       " 'eval_steps_per_second': 6.008,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('For Batch Size = {}, and Learning Rate = {} :'.format(batch_size_4, lr_4))\n",
    "print('The Evaluation Metric on the validation Data: ')\n",
    "eval_metric_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd79a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_setting_2():\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels )\n",
    "    model=model.to(device)\n",
    "    \n",
    "    batch_size = 16\n",
    "    lr = 1e-6\n",
    "    print('Batch size Learning Rate')\n",
    "    print(batch_size, lr)\n",
    "    \n",
    "    (trainer, train_result, eval_metric) = training_model(batch_size, lr, model)\n",
    "    \n",
    "    return  (trainer, train_result, eval_metric, batch_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f810eb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size Learning Rate\n",
      "16 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [939/939 06:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.933500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory output_dir/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 12:03:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/user/anaconda3/envs/env1/lib/python3.12/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Batch Size = 16, and Learning Rate = 1e-06 :\n",
      "The Evaluation Metric on the validation Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7791699171066284,\n",
       " 'eval_LOC_precision': 0.0,\n",
       " 'eval_LOC_recall': 0.0,\n",
       " 'eval_LOC_f1': 0.0,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.0,\n",
       " 'eval_ORG_recall': 0.0,\n",
       " 'eval_ORG_f1': 0.0,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.0,\n",
       " 'eval_PER_recall': 0.0,\n",
       " 'eval_PER_f1': 0.0,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.0,\n",
       " 'eval_overall_recall': 0.0,\n",
       " 'eval_overall_f1': 0.0,\n",
       " 'eval_overall_accuracy': 0.8187198114551364,\n",
       " 'eval_runtime': 17.4745,\n",
       " 'eval_samples_per_second': 278.062,\n",
       " 'eval_steps_per_second': 8.698,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainer_5, train_result_5, eval_metric_5, batch_size_5, lr_5) = hyper_param_setting_2()\n",
    "print ('For Batch Size = {}, and Learning Rate = {} :'.format(batch_size_5, lr_5))\n",
    "print('The Evaluation Metric on the validation Data: ')\n",
    "eval_metric_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35c1a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_4.model.save_pretrained(\"./IndicNER_trainer_4\", from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07b52d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_5.model.save_pretrained(\"./IndicNER_trainer_5\", from_pt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171422ba",
   "metadata": {},
   "source": [
    "### Calculating Macro-F1 for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10f1a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trainer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b68fbed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metric = best_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "341fffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.model.save_pretrained(\"./IndicBERT_best\", from_pt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b305a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16460365056991577,\n",
       " 'eval_LOC_precision': 0.7739938080495357,\n",
       " 'eval_LOC_recall': 0.7552870090634441,\n",
       " 'eval_LOC_f1': 0.7645259938837919,\n",
       " 'eval_LOC_number': 331,\n",
       " 'eval_ORG_precision': 0.7116279069767442,\n",
       " 'eval_ORG_recall': 0.7391304347826086,\n",
       " 'eval_ORG_f1': 0.7251184834123222,\n",
       " 'eval_ORG_number': 207,\n",
       " 'eval_PER_precision': 0.8315789473684211,\n",
       " 'eval_PER_recall': 0.8605664488017429,\n",
       " 'eval_PER_f1': 0.8458244111349036,\n",
       " 'eval_PER_number': 459,\n",
       " 'eval_overall_precision': 0.7877591312931885,\n",
       " 'eval_overall_recall': 0.8004012036108324,\n",
       " 'eval_overall_f1': 0.7940298507462685,\n",
       " 'eval_overall_accuracy': 0.9548401774542146,\n",
       " 'eval_runtime': 3.2695,\n",
       " 'eval_samples_per_second': 185.657,\n",
       " 'eval_steps_per_second': 5.811,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d745cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.19311270117759705,\n",
       " 'eval_LOC_precision': 0.7546480743691899,\n",
       " 'eval_LOC_recall': 0.8086090359302739,\n",
       " 'eval_LOC_f1': 0.7806972351021809,\n",
       " 'eval_LOC_number': 2811,\n",
       " 'eval_ORG_precision': 0.6146688560481663,\n",
       " 'eval_ORG_recall': 0.6413478012564249,\n",
       " 'eval_ORG_f1': 0.6277249860257126,\n",
       " 'eval_ORG_number': 1751,\n",
       " 'eval_PER_precision': 0.8009319181982915,\n",
       " 'eval_PER_recall': 0.8366684694429422,\n",
       " 'eval_PER_f1': 0.8184102631926993,\n",
       " 'eval_PER_number': 3698,\n",
       " 'eval_overall_precision': 0.745805561939784,\n",
       " 'eval_overall_recall': 0.7857142857142857,\n",
       " 'eval_overall_f1': 0.7652399481193256,\n",
       " 'eval_overall_accuracy': 0.9458969478005258,\n",
       " 'eval_runtime': 24.9836,\n",
       " 'eval_samples_per_second': 194.487,\n",
       " 'eval_steps_per_second': 6.084,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric = best_model.evaluate(val_data)\n",
    "val_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dab02643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11506272107362747,\n",
       " 'eval_LOC_precision': 0.8161290322580645,\n",
       " 'eval_LOC_recall': 0.8566591422121896,\n",
       " 'eval_LOC_f1': 0.8359030837004405,\n",
       " 'eval_LOC_number': 11518,\n",
       " 'eval_ORG_precision': 0.7471780225758194,\n",
       " 'eval_ORG_recall': 0.7658210203512684,\n",
       " 'eval_ORG_f1': 0.7563846630412335,\n",
       " 'eval_ORG_number': 7174,\n",
       " 'eval_PER_precision': 0.8539766157008866,\n",
       " 'eval_PER_recall': 0.8851967769860825,\n",
       " 'eval_PER_f1': 0.8693064774547953,\n",
       " 'eval_PER_number': 15017,\n",
       " 'eval_overall_precision': 0.8184752492216287,\n",
       " 'eval_overall_recall': 0.8500400486516954,\n",
       " 'eval_overall_f1': 0.8339590791350155,\n",
       " 'eval_overall_accuracy': 0.9645699033037872,\n",
       " 'eval_runtime': 103.5324,\n",
       " 'eval_samples_per_second': 193.176,\n",
       " 'eval_steps_per_second': 6.037,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metric = best_model.evaluate(train_data)\n",
    "train_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9cb5b2",
   "metadata": {},
   "source": [
    "### Load the Manually Annotated Data and Calc the Macro-F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d08f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_corpus_label_from_manual_anno(text):\n",
    "    \"\"\"\n",
    "    Modify the annotated label, such that model can use it as Tags\n",
    "    \"\"\"\n",
    "    text_1 = text.split(' ')\n",
    "    new_text = []\n",
    "    new_id = []\n",
    "    for word in text_1:\n",
    "        if word == '[' or word == ']' or word == ',' or word == '\"' or word == '\\n' or word == ' ' or word == '':\n",
    "            pass\n",
    "        else:\n",
    "            n_w = ''\n",
    "            for i in word:\n",
    "                if i == '[' or i == ']' or i == ',' or i == '\"' or i == '\\n' or i == '\\'':\n",
    "                    pass\n",
    "                else:\n",
    "                    n_w = n_w + i\n",
    "            if n_w != '':\n",
    "                new_text.append(n_w)\n",
    "                id1 = label_to_id_dict[n_w]\n",
    "                if id1 == 7 or id1 == 8:\n",
    "                    id1 = 0\n",
    "                new_id.append(id1)\n",
    "    return (new_id, new_text)\n",
    "\n",
    "\n",
    "i = 0\n",
    "text_corpus = {'words':[],\n",
    "              'ner' : []}\n",
    "with open(\"annotated_text.txt\", 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if i % 3 == 0:\n",
    "            text = line\n",
    "            text_1 = text.split(\" \")[1:]\n",
    "            if text_1[-1] == '\\n':\n",
    "                text_1 = text_1[0:-1]\n",
    "            text_corpus['words'].append(text_1)\n",
    "        elif i % 3 == 1:\n",
    "            label = line\n",
    "            ( new_label_id, new_label ) = form_corpus_label_from_manual_anno(label)\n",
    "            text_corpus['ner'].append(new_label_id)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c443c36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': [['রাত',\n",
       "   'সাড়ে',\n",
       "   'আটটাতেও',\n",
       "   'ওকে',\n",
       "   'একা',\n",
       "   'রিকশায়',\n",
       "   'ছেড়ে',\n",
       "   'দিতে',\n",
       "   'তিনি',\n",
       "   'রাজি',\n",
       "   'নন।\\n'],\n",
       "  ['একাধিক',\n",
       "   'জঙ্গি',\n",
       "   'ঘাঁটি',\n",
       "   'ধ্বংস',\n",
       "   'করে',\n",
       "   'দেওয়া',\n",
       "   'হয়েছে',\n",
       "   'বলে',\n",
       "   'দাবি',\n",
       "   'করে',\n",
       "   'সরকার।\\n'],\n",
       "  ['এ',\n",
       "   'সম্মেলনে',\n",
       "   'বিশ্বের',\n",
       "   'প্রায়',\n",
       "   'দেড়',\n",
       "   'হাজার',\n",
       "   'নারী',\n",
       "   'নেতৃত্ব',\n",
       "   'যোগদান',\n",
       "   'করেন।\\n'],\n",
       "  ['আব্দুল',\n",
       "   'মালেক',\n",
       "   'হিমু,',\n",
       "   'অপু',\n",
       "   'আলম,',\n",
       "   'দবির',\n",
       "   'মোহাম্মদ',\n",
       "   'সহ',\n",
       "   'আরো',\n",
       "   'অনেকে।\\n'],\n",
       "  ['মিথিলা',\n",
       "   'যে',\n",
       "   'অনিকের',\n",
       "   'এই',\n",
       "   'জিনিসগুলো',\n",
       "   'জানে',\n",
       "   'না',\n",
       "   'তা',\n",
       "   'নয়।',\n",
       "   'কিন্তু',\n",
       "   'সে',\n",
       "   'চায়',\n",
       "   'যে',\n",
       "   'অনিক',\n",
       "   'সব',\n",
       "   'সময়',\n",
       "   'তার',\n",
       "   'কাছে',\n",
       "   'সত্যি',\n",
       "   'কথা',\n",
       "   'বলুক।'],\n",
       "  ['লোকসভার',\n",
       "   'স্পিকার',\n",
       "   'ওম',\n",
       "   'বিড়লার',\n",
       "   'প্রস্তাবে',\n",
       "   'সম্মতি',\n",
       "   'জানিয়েছেন',\n",
       "   'সব',\n",
       "   'দলের',\n",
       "   'সাংসদরা।\\n'],\n",
       "  ['মিথিলা', 'চুপ', 'করে', 'শুনে', 'গেলেও', 'মুখে', 'কিছু', 'বলল', 'না।'],\n",
       "  ['জানা',\n",
       "   'যায়,',\n",
       "   'জগন্নাথপুর',\n",
       "   'পৌর',\n",
       "   'শহরের',\n",
       "   'ছিলিমপুর',\n",
       "   'গ্রামের',\n",
       "   'প্রয়াত',\n",
       "   'শালিসি',\n",
       "   'ব্যক্তি',\n",
       "   'হাজী',\n",
       "   'ফিরোজ',\n",
       "   'মিয়ার',\n",
       "   'বিপুল',\n",
       "   'পরিমাণ',\n",
       "   'জায়গা',\n",
       "   'সম্পত্তি',\n",
       "   'রয়েছে।'],\n",
       "  ['পুলিশ',\n",
       "   'সুপার',\n",
       "   'শ্যামল',\n",
       "   'কুমার',\n",
       "   'নাথ',\n",
       "   'জানান,',\n",
       "   'জমিসংক্রান্ত',\n",
       "   'বিষয়',\n",
       "   'নিয়ে',\n",
       "   'এই',\n",
       "   'হামলা',\n",
       "   'হয়েছে।'],\n",
       "  ['এ',\n",
       "   'ছবিতে',\n",
       "   'সলমন',\n",
       "   'খানকে',\n",
       "   'এক',\n",
       "   'সাধারণ',\n",
       "   'নাগরিকের',\n",
       "   'চরিত্রে',\n",
       "   'দেখা',\n",
       "   'যাবে।\\n'],\n",
       "  ['একাদশ',\n",
       "   'জাতীয়',\n",
       "   'সংসদ',\n",
       "   'নির্বাচনে',\n",
       "   'নির্বাচিত',\n",
       "   '২৯০',\n",
       "   'সাংসদের',\n",
       "   'পদে',\n",
       "   'থাকার',\n",
       "   'বৈধতা',\n",
       "   'নিয়ে',\n",
       "   'করা',\n",
       "   'রিটটি',\n",
       "   'সরাসরি',\n",
       "   'খারিজ',\n",
       "   'করে',\n",
       "   'দিয়েছেন',\n",
       "   'হাইকোর্ট।\\n'],\n",
       "  ['টাকা',\n",
       "   'ছিল',\n",
       "   'না',\n",
       "   'বলে',\n",
       "   'জিনিসপত্র',\n",
       "   'ফেরি',\n",
       "   'করে',\n",
       "   'বেড়ানো,',\n",
       "   'দোকান',\n",
       "   'দেওয়া,',\n",
       "   'থাকা-খাওয়ার',\n",
       "   'বিনিময়ে',\n",
       "   'নবিশি',\n",
       "   'কিংবা',\n",
       "   'সমাজের',\n",
       "   'উঁচুতলার',\n",
       "   'জন্য',\n",
       "   'সন্ন্যাসিনী',\n",
       "   'হওয়া—এসব',\n",
       "   'বাতিল',\n",
       "   'হয়ে',\n",
       "   'গেল।\\n'],\n",
       "  ['কারণ',\n",
       "   'সে',\n",
       "   'নিজে',\n",
       "   'কাল',\n",
       "   'অনিককে',\n",
       "   'শপিং',\n",
       "   'মলে',\n",
       "   'একটা',\n",
       "   'মেয়ের',\n",
       "   'সাথে',\n",
       "   'দেখেছে।'],\n",
       "  ['মেয়েটি',\n",
       "   'যাতে',\n",
       "   'বিনা',\n",
       "   'মূল্যে',\n",
       "   'লেখাপড়ার',\n",
       "   'সুযোগ',\n",
       "   'পায়,',\n",
       "   'সে',\n",
       "   'ব্যবস্থাও',\n",
       "   'সরকারকেই',\n",
       "   'করতে',\n",
       "   'হবে।\\n'],\n",
       "  ['জানা',\n",
       "   'গেছে,',\n",
       "   'বিপিএলের',\n",
       "   'উদ্বোধনী',\n",
       "   'অনুষ্ঠানে',\n",
       "   'সালমান',\n",
       "   'খান',\n",
       "   'নাকি',\n",
       "   'শুরুতে',\n",
       "   'ঢাকায়',\n",
       "   'আসতে',\n",
       "   'রাজি',\n",
       "   'হতে',\n",
       "   'হননি।\\n'],\n",
       "  ['কঠোর',\n",
       "   'অধ্যবসায়',\n",
       "   'এবং',\n",
       "   'অক্লান্ত',\n",
       "   'পরিশ্রমের',\n",
       "   'ফলে',\n",
       "   'বিদেশে',\n",
       "   'বাংলাদেশিদের',\n",
       "   'সাফল্যের',\n",
       "   'নজির',\n",
       "   'এখন',\n",
       "   'ভুরি',\n",
       "   'ভুরি।'],\n",
       "  ['দাতব্য',\n",
       "   'প্রতিষ্ঠানের',\n",
       "   'হাজার',\n",
       "   'হাজার',\n",
       "   'পাউন্ড',\n",
       "   'সংগ্রহের',\n",
       "   'কৃতিত্বের',\n",
       "   'জন্য',\n",
       "   'তাকে',\n",
       "   'এই',\n",
       "   'স্বীকৃতি',\n",
       "   'দেওয়া',\n",
       "   'হয়েছে।\\n'],\n",
       "  ['লাখ',\n",
       "   'লাখ',\n",
       "   'নারীর',\n",
       "   'স্বার্থে',\n",
       "   'আমরা',\n",
       "   'অবশ্যই',\n",
       "   'আমাদের',\n",
       "   'অভিন্ন',\n",
       "   'সংস্কৃতি,',\n",
       "   'ঐতিহ্য',\n",
       "   'এবং',\n",
       "   'মূল্যবোধ',\n",
       "   'নিয়ে',\n",
       "   'একত্রে',\n",
       "   'কাজ',\n",
       "   'করতে',\n",
       "   'হবে।’'],\n",
       "  ['সম্ভীরের',\n",
       "   'বিরুদ্ধে',\n",
       "   '২৭৮',\n",
       "   'পাতার',\n",
       "   'চার্জশিট',\n",
       "   'পেশ',\n",
       "   'করার',\n",
       "   'কথা',\n",
       "   'থাকলেও',\n",
       "   'তা',\n",
       "   'পেশ',\n",
       "   'করতে',\n",
       "   'ব্যর্থ',\n",
       "   'পুলিশের',\n",
       "   'গোয়েন্দা',\n",
       "   'বিভাগ।\\n'],\n",
       "  ['৫',\n",
       "   'এপ্রিল',\n",
       "   '২০১৯',\n",
       "   '০০:০০',\n",
       "   '|',\n",
       "   'আপডেট:',\n",
       "   '৫',\n",
       "   'এপ্রিল',\n",
       "   '২০১৯',\n",
       "   '০২:৪০\\n'],\n",
       "  ['পরিষ্কার',\n",
       "   'বলে',\n",
       "   'দিলেন',\n",
       "   '‘ভদ্রলোকের',\n",
       "   'খেলা’',\n",
       "   'ক্রিকেটে',\n",
       "   'এর',\n",
       "   'চেতনাবিরোধী',\n",
       "   'কোনো',\n",
       "   'কিছুতে',\n",
       "   'ছাড়',\n",
       "   'দেবে',\n",
       "   'না',\n",
       "   'আইসিসি।'],\n",
       "  ['তাকে',\n",
       "   'উদ্ধার',\n",
       "   'করে',\n",
       "   'তারকেশ্বর',\n",
       "   'গ্রামীণ',\n",
       "   'হাসপাতালে',\n",
       "   'নিয়ে',\n",
       "   'যাওয়া',\n",
       "   'হয়।'],\n",
       "  ['তিনি',\n",
       "   'বলেন,',\n",
       "   'মহান',\n",
       "   'আল্লাহ',\n",
       "   \"তা'আলার\",\n",
       "   'বাণী',\n",
       "   ':',\n",
       "   '“তাঁরা',\n",
       "   'যাদেরকে',\n",
       "   'আহ্বান',\n",
       "   'করে',\n",
       "   'তারাই',\n",
       "   'তো',\n",
       "   'তাদের',\n",
       "   'প্রতিপালকের',\n",
       "   'নৈকটা',\n",
       "   'লাভের',\n",
       "   'উপায়',\n",
       "   'খোজ',\n",
       "   'করে\"-',\n",
       "   '(সূরা',\n",
       "   'আল',\n",
       "   'ইসরা',\n",
       "   '১৭:৫৭)',\n",
       "   'এর',\n",
       "   'ব্যাখ্যায়',\n",
       "   'বলেন,',\n",
       "   'একদা',\n",
       "   'একদল',\n",
       "   'জিন',\n",
       "   'ইসলাম',\n",
       "   'গ্রহণ',\n",
       "   'করলো।',\n",
       "   '(একদল',\n",
       "   'মানুষ)',\n",
       "   'তাদের',\n",
       "   '(জিনদের)',\n",
       "   'পূজা',\n",
       "   'করতো।',\n",
       "   'কিন্তু',\n",
       "   'পূজায়',\n",
       "   'রত',\n",
       "   'এ',\n",
       "   'লোকগুলো',\n",
       "   'তাদের',\n",
       "   'পূজাতেই',\n",
       "   'অটল',\n",
       "   'থাকল।',\n",
       "   'অথচ',\n",
       "   'জিনের',\n",
       "   'একদল',\n",
       "   'ইসলাম',\n",
       "   'গ্রহণ',\n",
       "   'করেছে।',\n",
       "   '(ই.ফা',\n",
       "   '৭২৭৩,',\n",
       "   'ই.সে',\n",
       "   '৭৩২৮)\\n'],\n",
       "  ['হাজার',\n",
       "   'হাজার',\n",
       "   'নারী',\n",
       "   'বর্মী',\n",
       "   'সেনাবাহিনী',\n",
       "   'ও',\n",
       "   'মগদস্যুদের',\n",
       "   'কাছে',\n",
       "   'সম্ভ্রম',\n",
       "   'হারিয়েছে।',\n",
       "   'গণধর্ষণের',\n",
       "   'পর',\n",
       "   'অনেককে',\n",
       "   'নির্মম',\n",
       "   'ও',\n",
       "   'নিষ্ঠুরভাবে',\n",
       "   'হত্যাও',\n",
       "   'করা',\n",
       "   'হয়েছে।\\n'],\n",
       "  ['অন্যান্য',\n",
       "   'হোটেল',\n",
       "   'মালিকদের',\n",
       "   'মতে,',\n",
       "   'এই',\n",
       "   'অশান্ত',\n",
       "   'পরিস্থিতির',\n",
       "   'মধ্যে',\n",
       "   'পর্যটকদের',\n",
       "   'পাশে',\n",
       "   'থাকাই',\n",
       "   'তাদের',\n",
       "   'কর্তৃব্য',\n",
       "   'বলে',\n",
       "   'মনে',\n",
       "   'করছেন',\n",
       "   'তারা।\\n']],\n",
       " 'ner': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 2, 2, 1, 2, 1, 2, 0, 0, 0],\n",
       "  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 3, 0, 0, 1, 2, 0, 0, 5, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3],\n",
       "  [0, 0, 0, 5, 0, 0, 0, 0, 0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5903f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(text_corpus['ner']), len(text_corpus['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42117217",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_corpus['ner'])):\n",
    "    (j, k) = (len(text_corpus['words'][i]), len(text_corpus['ner'][i]))\n",
    "    if j > k:\n",
    "        for _ in range(j - k):\n",
    "            text_corpus['ner'][i].append(0)\n",
    "    elif j < k :\n",
    "        text_corpus['ner'][i] = text_corpus['ner'][i][0 : k - (k - j)]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3639f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_anno_data = Dataset.from_dict(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "849fb3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'ner'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_anno_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33c2e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on validation dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 1180.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "manual_anno_data = manual_anno_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "974f583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_manual_anno = best_model.evaluate(manual_anno_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587b9463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0887410044670105,\n",
       " 'eval_LOC_precision': 0.8,\n",
       " 'eval_LOC_recall': 1.0,\n",
       " 'eval_LOC_f1': 0.888888888888889,\n",
       " 'eval_LOC_number': 4,\n",
       " 'eval_ORG_precision': 0.4,\n",
       " 'eval_ORG_recall': 1.0,\n",
       " 'eval_ORG_f1': 0.5714285714285715,\n",
       " 'eval_ORG_number': 2,\n",
       " 'eval_PER_precision': 0.9333333333333333,\n",
       " 'eval_PER_recall': 0.875,\n",
       " 'eval_PER_f1': 0.9032258064516129,\n",
       " 'eval_PER_number': 16,\n",
       " 'eval_overall_precision': 0.8,\n",
       " 'eval_overall_recall': 0.9090909090909091,\n",
       " 'eval_overall_f1': 0.8510638297872342,\n",
       " 'eval_overall_accuracy': 0.9792207792207792,\n",
       " 'eval_runtime': 0.2025,\n",
       " 'eval_samples_per_second': 123.481,\n",
       " 'eval_steps_per_second': 4.939,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_manual_anno"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
