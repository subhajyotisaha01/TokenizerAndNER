{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18a2d53-ff00-4079-bd18-092c9d274171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c02bc407-ba27-4531-88dd-9773c5818f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_o_consonants = {\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'ঙ',\n",
    "    'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ',\n",
    "    'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম',\n",
    "    'য', 'য়', 'র', 'ল',\n",
    "    'শ', 'ষ', 'স', 'হ',\n",
    "    'ক়', 'খ়', 'গ়', 'জ়', 'ফ়'\n",
    "}\n",
    "# set_o_punctuations = {\n",
    "#     '!', '।', '\\n', ',', '-', '(', ')', '?', '.'\n",
    "# }\n",
    "set_o_vowels = {\n",
    "    'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ',\n",
    "    'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'\n",
    "}\n",
    "set_o_special_tokens = {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "set_o_independent_vowels = {'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ'}\n",
    "set_o_dependent_vowels = { 'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebfc8718-791b-4ac1-8e65-88688e884be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "    \n",
    "def find_unigram_freq(corpus):\n",
    "    # Finding the unigram frequency of each element in the corpus\n",
    "    uni_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != ' ': # Removing the space\n",
    "                if line[j] in uni_dict.keys():\n",
    "                    # Key already existed\n",
    "                    uni_dict[line[j]] = uni_dict[line[j]] + 1\n",
    "                else:\n",
    "                    # New key added\n",
    "                    uni_dict[line[j]] = 0\n",
    "\n",
    "    keys_list = list(uni_dict.keys())\n",
    "    values_list = list(uni_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "    # Returning Key and Sorted Index based upon the Value of the frequency of the occurence of the character\n",
    "    return (keys_list, sorted_value_index)\n",
    "    \n",
    "def form_bigram_freq(corpus):\n",
    "    # Corpus is a list of list, which has a list corresponding to each line and all the lines are in another list\n",
    "    bigram_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line) - 1):\n",
    "            if line[j] != ' ' and line[j+1] != ' ' and line[j] != '#':\n",
    "                if (line[j], line[j+1]) in bigram_dict.keys():\n",
    "                    # Increasing the count\n",
    "                    bigram_dict[(line[j], line[j+1])] = bigram_dict[(line[j], line[j+1])] + 1\n",
    "                else:\n",
    "                    # New Key Found\n",
    "                    bigram_dict[(line[j], line[j+1])] = 0\n",
    "\n",
    "    keys_list = list(bigram_dict.keys())\n",
    "    values_list = list(bigram_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    # Returning Key List and its Sorted index based upon the value\n",
    "    return (keys_list, sorted_value_index)\n",
    "\n",
    "def syllable_form_from_line(text):\n",
    "\n",
    "    syllable_list = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "\n",
    "        n = len(text[i])\n",
    "        if n == 1:\n",
    "            syllable_list.append(text[i])\n",
    "            i = i+ 1\n",
    "        else:\n",
    "            j = 0\n",
    "            while j<len(text[i]):\n",
    "                syllable = ''\n",
    "                while text[i][j] == '#' and (j+1)<len(text[i]):\n",
    "                    # The Hash Char are avoided while calculating syllable\n",
    "                    j = j + 1\n",
    "                if text[i][j] in set_o_independent_vowels:\n",
    "                    # Independent vowels will be a point of split for syllable\n",
    "                    syllable = syllable + text[i][j]\n",
    "                elif text[i][j] in set_o_consonants:\n",
    "                    # Consonant after Vowel will start a new syllable\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    if (j+1) < len(text[i]) :\n",
    "                        if text[i][j+1] in set_o_consonants or text[i][j+1] == 'অ':\n",
    "                            # The following consonants after a consonants or the 'অ'\n",
    "                            # will remain in the same syllable\n",
    "                            syllable = syllable + text[i][j+1]\n",
    "                            j = j + 1\n",
    "                j = j + 1\n",
    "                while j < len(text[i]) and text[i][j] in set_o_dependent_vowels:\n",
    "                    # Dependent Vowels will be added with previous syllable\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    j = j + 1\n",
    "                if syllable != '':\n",
    "                    syllable_list.append(syllable)\n",
    "            i = i + 1\n",
    "\n",
    "    return syllable_list\n",
    "\n",
    "def syllable_form_from_whole_corpus(unicode_corrected_corpus):\n",
    "    \"\"\"\n",
    "    Args: A List of List of Unicode corrected Chars corresponding to each sentence, words are separated by ' '\n",
    "    \"\"\"\n",
    "    syllable_corpus = []\n",
    "    for i in range(len(unicode_corrected_corpus)):\n",
    "        syllable_corpus.append(syllable_form_from_line(unicode_corrected_corpus[i]))\n",
    "    return syllable_corpus\n",
    "    \n",
    "def form_char_corpus_from_token(token_list_1k):\n",
    "    \"\"\"\n",
    "    The functuion splits every tokens into characters to find the frequency of the characters\n",
    "    \"\"\"\n",
    "    char_corpus = []\n",
    "    for i in range(len(token_list_1k)):\n",
    "        char_line_corpus = []\n",
    "        for j in range(len(token_list_1k[i])):\n",
    "            if token_list_1k[i][j] not in set_o_special_tokens:\n",
    "                # Ignoring pecial token and '_' token corresponding to space\n",
    "                text = token_list_1k[i][j].replace('▁', '')\n",
    "                char_line_corpus += list(text)\n",
    "        char_corpus.append(char_line_corpus)\n",
    "\n",
    "    return char_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a7db0f-2d02-404d-8c84-d2d3d6e8a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\") # IndicBERT Pre-tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a442ab2-35ee-44ab-9668-3fd2437277eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text, max_length):\n",
    "    # Function to tokenize the text\n",
    "    token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)\n",
    "    tokenized_line = [tokenizer.convert_ids_to_tokens(token_id) for token_id in token_ids]\n",
    "    return tokenized_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d4c6114-2060-4957-be70-c47a976ea537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "য  র  ▁  ।  [SEP]  [CLS]  ন  ক  ত  ▁পর  ,  ট  ষ  ▁স  ▁কর  ▁দ  ব  ▁ব  ল  ড  "
     ]
    }
   ],
   "source": [
    "token_list_corpus = []\n",
    "max_len = 1000 # Maxlen = 1k\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        # print(line)\n",
    "        token_list_corpus.append(tokenize_text(tokenizer, line, max_length))\n",
    "# Unigram Tokens\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Uni-gram Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e82207f-aa85-4538-b2c2-41090cf13435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "('▁', '[SEP]')  ('।', '▁')  ('▁হ', 'য')  ('য', 'র')  ('▁থ', 'ক')  ('▁', 'র')  ('য', 'ন')  ('য', 'ছ')  ('▁ন', 'য')  ('য', '।')  ('ছ', '।')  ('▁ব', 'য')  ('ন', '।')  ('ন', ',')  ('▁জন', 'য')  ('র', '▁পর')  ('▁দ', 'য')  ('র', '▁')  ('ঙ', 'গ')  ('▁এক', 'ট')  "
     ]
    }
   ],
   "source": [
    "# Bigram Tokens\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70a825b4-e6a9-4150-91a7-6a065d39d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68c8df0e-6ed1-4c5b-b582-1204268a2fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Bi-gram Frequency Syllable:\n",
      "('।', '▁') ('য', 'র') ('হ', 'য') ('ন', 'য') ('য', 'ন') ('এ', 'ক') ('থ', 'ক') ('ন', '।') ('▁', 'র') ('ব', 'য') ('ছ', '।') ('ও', 'য') ('অ', 'ন') ('য', 'ছ') ('আ', 'ম') ('য', 'য') ('র', 'পর') ('দ', 'য') ('য', '।') ('ন', ',') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Syllable\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Bi-gram Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b960dd4-5087-4b57-b99e-5df6077e5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corpus = form_char_corpus_from_token(token_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47602280-5a6b-4d97-afba-dc3ac718e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('ক', 'র') ('প', 'র') ('ত', 'র') ('র', 'ক') ('র', 'ব') ('র', 'স') ('র', 'ম') ('ব', 'র') ('ন', 'ত') ('র', 'ত') ('ন', 'য') ('ন', 'র') ('র', 'প') ('দ', 'র') ('ম', 'ন') ('জ', 'ন') ('র', 'র') ('র', 'ন') ('ব', 'ল') ('ন', 'ক') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Char\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0fc5f-ac3d-4fdc-bbcd-ea6a4d696a1e",
   "metadata": {},
   "source": [
    "## IndicBERT with 2k Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2e353da-ac53-4524-bea8-63fcc27fb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "য  র  ▁  ।  [SEP]  [CLS]  ন  ক  ত  ▁পর  ,  ট  ষ  ▁স  ▁কর  ▁দ  ব  ▁ব  ল  ড  "
     ]
    }
   ],
   "source": [
    "token_list_corpus = []\n",
    "max_len = 2000\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        # print(line)\n",
    "        token_list_corpus.append(tokenize_text(tokenizer, line, max_length))\n",
    "# Uni-gram Tokens\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4df2083-9fe9-445a-86b2-bbcc15dbfea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "('▁', '[SEP]')  ('।', '▁')  ('▁হ', 'য')  ('য', 'র')  ('▁থ', 'ক')  ('▁', 'র')  ('য', 'ন')  ('য', 'ছ')  ('▁ন', 'য')  ('য', '।')  ('ছ', '।')  ('▁ব', 'য')  ('ন', '।')  ('ন', ',')  ('▁জন', 'য')  ('র', '▁পর')  ('▁দ', 'য')  ('র', '▁')  ('ঙ', 'গ')  ('▁এক', 'ট')  "
     ]
    }
   ],
   "source": [
    "# Bi-gram Tokens\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f39b99e-cf78-414d-870e-97784904e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2d0784-79bf-4020-93bd-f4768f385f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Bi-gram Frequency Syllable:\n",
      "('।', '▁') ('য', 'র') ('হ', 'য') ('ন', 'য') ('য', 'ন') ('এ', 'ক') ('থ', 'ক') ('ন', '।') ('▁', 'র') ('ব', 'য') ('ছ', '।') ('ও', 'য') ('অ', 'ন') ('য', 'ছ') ('আ', 'ম') ('য', 'য') ('র', 'পর') ('দ', 'য') ('য', '।') ('ন', ',') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Frequent Syllables\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Bi-gram Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e864e158-70f7-4da3-9496-1c8dfb5e6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corpus = form_char_corpus_from_token(token_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa5fa5b1-4878-403d-a1c4-3891f1325e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('ক', 'র') ('প', 'র') ('ত', 'র') ('র', 'ক') ('র', 'ব') ('র', 'স') ('র', 'ম') ('ব', 'র') ('ন', 'ত') ('র', 'ত') ('ন', 'য') ('ন', 'র') ('র', 'প') ('দ', 'র') ('ম', 'ন') ('জ', 'ন') ('র', 'র') ('র', 'ন') ('ব', 'ল') ('ন', 'ক') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Frequent Characters\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a315d-f98a-4440-9fa7-333e9edca88c",
   "metadata": {},
   "source": [
    "## Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256a4809-251e-4dc7-91b5-65295751988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the earlier given corpus by mBERT Tokenizer\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "max_len = 1000\n",
    "token_list_1k = []\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    for i in range(n_lines):\n",
    "            line = f.readline()\n",
    "            if i % 3 == 0:\n",
    "                token_list_1k.append(tokenize_text(tokenizer, line, max_len)) # Reading the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3783ac68-76b9-4b26-a14e-c11b09160755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁1.', '▁কন', 'ত', '▁স', 'টর', '▁গ', 'য', '▁হত', '▁দ', 'য', '▁তন', '▁আত', 'ক', '▁ওঠ', 'ন', '।', '▁', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab718816-435d-4201-9b83-0e2bf97712b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision, recall and F1 Score\n",
    "def metric_per_line(output_token, ground_truth_token):\n",
    "    output_token = set(output_token)\n",
    "    ground_truth_token = set(ground_truth_token)\n",
    "    true_positive = len( output_token & ground_truth_token)\n",
    "    false_positive = len( output_token - ground_truth_token)\n",
    "    false_negative = len(ground_truth_token - output_token)\n",
    "\n",
    "    if (true_positive + false_positive) > 0:\n",
    "        precision = true_positive / ( true_positive + false_positive )\n",
    "    else:\n",
    "        precision = 0\n",
    "    if ( true_positive + false_negative) > 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1_score = 2 / ((1/precision) + (1 / recall))\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    return (precision, recall, f1_score)\n",
    "\n",
    "def mean_metric_calculate(output_tokens, ground_truth_txt_filename = 'cs689_assignment.txt'):\n",
    "\n",
    "    n_lines = len(output_tokens) # Num of Lines\n",
    "    (sum_precision, sum_recall, sum_f1_score) = (0, 0, 0)\n",
    "    with open(ground_truth_txt_filename, 'r') as f:\n",
    "        for i in range(n_lines):\n",
    "            temp = f.readline() # Removing the text line\n",
    "            ground_truth = f.readline()  # Reading the even number sentences of the labels\n",
    "            temp = f.readline()\n",
    "            (t_p, t_r, t_f) = metric_per_line(output_tokens[i], ground_truth)\n",
    "            sum_precision += t_p\n",
    "            sum_recall += t_r\n",
    "            sum_f1_score += t_f\n",
    "    (mean_precision, mean_recall, mean_f1_score) = (sum_precision / n_lines, sum_recall / n_lines, sum_f1_score / n_lines)\n",
    "\n",
    "    return (mean_precision, mean_recall, mean_f1_score)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab007c7f-40e9-4c96-9e1f-9a58a2a801d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(preision, recall, f1_score) = mean_metric_calculate(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d61d5467-8ee3-41a9-ba50-f804af75c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.23911910735306321, Recall: 0.22415086687487434, F1 Score: 0.2304098277473331\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {preision}, Recall: {recall}, F1 Score: {f1_score}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc0172-cae6-4eb1-bb64-480891a82a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
