{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JI-znyxbVL7P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QWMDdq1bU15s"
   },
   "outputs": [],
   "source": [
    "set_o_consonants = {\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'ঙ',\n",
    "    'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ',\n",
    "    'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম',\n",
    "    'য', 'য়', 'র', 'ল',\n",
    "    'শ', 'ষ', 'স', 'হ',\n",
    "    'ক়', 'খ়', 'গ়', 'জ়', 'ফ়'\n",
    "}\n",
    "# set_o_punctuations = {\n",
    "#     '!', '।', '\\n', ',', '-', '(', ')', '?', '.'\n",
    "# }\n",
    "set_o_vowels = {\n",
    "    'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ',\n",
    "    'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'\n",
    "}\n",
    "set_o_special_tokens = {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "set_o_independent_vowels = {'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ'}\n",
    "set_o_dependent_vowels = { 'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppaPVVUXP9_m",
    "outputId": "5a366f3d-392b-4a3d-f7b4-fc3d3e8268da"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def mBERT_tokenize_text(line, max_len):\n",
    "    # M-BERT tokenization\n",
    "    token_nums = tokenizer.encode(line, truncation = True, max_length = max_len, add_special_tokens=True)\n",
    "    tokenized_line = [tokenizer.convert_ids_to_tokens(token_id) for token_id in token_nums]\n",
    "    return tokenized_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MS4aqtRfPBLe"
   },
   "outputs": [],
   "source": [
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "max_len = 1000\n",
    "token_list_1k = []\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        token_list_1k.append(mBERT_tokenize_text(line, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CwONbz5FR2OI",
    "outputId": "8a730903-06c3-4fac-adfb-97ad624245f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339479"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZVd3k0xUfPS",
    "outputId": "17305b10-a1f8-4955-dba9-13a216069334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'গ', '##্রে', '##প্ত', '##ার', '##ক', '##ৃত', '##রা', 'হলো', ',', 'ক', '##ু', '##মি', '##ল', '##্', '##লা', 'জেলার', 'সদর', 'ক', '##ো', '##ত', '##য়া', '##লী', 'মডেল', 'থ', '##ানা', '##র', 'শ', '##াস', '##ন', '##গ', '##া', '##ছ', '##া', 'প', '##াল', '##প', '##াড়া', '[UNK]', 'স', '[UNK]', 'ম', '##িল', '##স', 'এলাকা', '##র', 'ই', '##উ', '##ন', '##ু', '##স', 'ম', '##িয়ার', 'ছ', '##েলে', 'ম', '##হ', '##াস', '##ীন', '(', '২৮', ')', ',', 'ম', '##ৃত', 'বা', '##হার', 'ম', '##িয়ার', 'স', '##্ত্রী', 'ফ', '##াতে', '##মা', 'ব', '##ে', '##গ', '##ম', 'আল', '##োন', '##ী', '(', '৪', '##২', ')', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K5Dawtk5U0E8"
   },
   "outputs": [],
   "source": [
    "def find_unigram_freq(corpus):\n",
    "    uni_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != ' ':\n",
    "                if line[j] in uni_dict.keys():\n",
    "                    uni_dict[line[j]] = uni_dict[line[j]] + 1\n",
    "                else:\n",
    "                    uni_dict[line[j]] = 0\n",
    "\n",
    "    keys_list = list(uni_dict.keys())\n",
    "    values_list = list(uni_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkr4JBQkVAgd",
    "outputId": "4f20fb12-4073-48ce-bcb0-86b287b41e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "।  ##র  [CLS]  [SEP]  ##ি  ##ে  স  ব  ##া  ##ন  প  ##ু  ,  ক  ##ম  ##ো  ##্  আ  ##ের  ##ক  "
     ]
    }
   ],
   "source": [
    "# Uni-gram Frequency of the Characters\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_1k)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s8PGj8L9Vm2r"
   },
   "outputs": [],
   "source": [
    "def form_bigram_freq(corpus):\n",
    "    bigram_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line) - 1):\n",
    "            if line[j] != ' ' and line[j+1] != ' ' and line[j] != '#':\n",
    "                if (line[j], line[j+1]) in bigram_dict.keys():\n",
    "                    bigram_dict[(line[j], line[j+1])] = bigram_dict[(line[j], line[j+1])] + 1\n",
    "                else:\n",
    "                    bigram_dict[(line[j], line[j+1])] = 0\n",
    "\n",
    "    keys_list = list(bigram_dict.keys())\n",
    "    values_list = list(bigram_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-aLMm69Vve0",
    "outputId": "1eaed089-e2bd-4ce6-d63c-fff950613b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Bi-gram Tokens are: \n",
      "('।', '[SEP]') ('প', '##্র') ('ব', '##ি') ('স', '##ম') ('##ম', '##্') ('ক', '##র') ('##ন', ',') ('স', '##ং') ('ন', '##ির') ('##্', '##প') ('পর', '##ি') ('দ', '##ে') ('##চ', '##্') ('প', '##া') ('ম', '##ু') ('##্', '##ষ') ('##া', '##ঁ') ('##ন', '।') ('জ', '##ানা') ('.', '.') "
     ]
    }
   ],
   "source": [
    " # Bi-gram Frequency of the Characters\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_1k)\n",
    "print('The Top-20 Bi-gram Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "v96K_eCzWDSY"
   },
   "outputs": [],
   "source": [
    "# Splitting sentence into syllables\n",
    "def syllable_form_from_line(text):\n",
    "\n",
    "    syllable_list = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "\n",
    "        n = len(text[i])\n",
    "        if n == 1:\n",
    "            syllable_list.append(text[i])\n",
    "            i = i+ 1\n",
    "        elif text[i] in set_o_special_tokens:\n",
    "            i = i + 1\n",
    "        else:\n",
    "            j = 0\n",
    "            while j<len(text[i]):\n",
    "                syllable = ''\n",
    "                while text[i][j] == '#' and (j+1)<len(text[i]):\n",
    "                    j = j + 1\n",
    "                if text[i][j] in set_o_independent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                elif text[i][j] in set_o_consonants:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    if (j+1) < len(text[i]) :\n",
    "                        if text[i][j+1] in set_o_consonants or text[i][j+1] == 'অ':\n",
    "                            syllable = syllable + text[i][j+1]\n",
    "                            j = j + 1\n",
    "                j = j + 1\n",
    "                while j < len(text[i]) and text[i][j] in set_o_dependent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    j = j + 1\n",
    "                if syllable != '':\n",
    "                    syllable_list.append(syllable)\n",
    "            i = i + 1\n",
    "\n",
    "    return syllable_list\n",
    "\n",
    "def syllable_form_from_whole_corpus(unicode_corrected_corpus):\n",
    "    \"\"\"\n",
    "    Args: A List of List of Unicode corrected Chars corresponding to each sentence, words are separated by ' '\n",
    "    \"\"\"\n",
    "    syllable_corpus = []\n",
    "    for i in range(len(unicode_corrected_corpus)):\n",
    "        syllable_corpus.append(syllable_form_from_line(unicode_corrected_corpus[i]))\n",
    "    return syllable_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7oMT6qCrcn2U"
   },
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_1k )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DqQBxE8gCDc",
    "outputId": "6a259ade-356f-4469-e7d6-f2e6f7b8a002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequency Syllable:\n",
      "('য', 'ে') ('য', 'া') ('প', 'র') ('ক', 'র') ('র', 'স') ('র', 'প') ('র', 'ক') ('ত', 'র') ('র', 'ব') ('দে', 'র') ('ন', 'ত') ('ন', 'র') ('স', 'ম') ('ন', '।') ('র', 'ম') ('ে', 'ছে') ('ও', 'য') ('এ', 'ক') ('দ', 'র') ('র', 'র') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Frequency Syllables\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UC6JO1Yqh18N"
   },
   "outputs": [],
   "source": [
    "def form_char_corpus_from_token(token_list_1k):\n",
    "\n",
    "    char_corpus = []\n",
    "    for i in range(len(token_list_1k)):\n",
    "        char_line_corpus = []\n",
    "        for j in range(len(token_list_1k[i])):\n",
    "            if token_list_1k[i][j] not in set_o_special_tokens:\n",
    "                text = token_list_1k[i][j].replace('##', '')\n",
    "                char_line_corpus += list(text)\n",
    "        char_corpus.append(char_line_corpus)\n",
    "\n",
    "    return char_corpus\n",
    "\n",
    "char_corpus = form_char_corpus_from_token(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "YbeEN1MBliYH",
    "outputId": "053e3758-85a5-47bb-c214-35b54c668210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('য', '়') ('া', 'র') ('ে', 'র') ('্', 'র') ('া', 'ন') ('্', 'য') ('র', 'া') ('র', 'ে') ('ন', '্') ('ে', 'ন') ('়', 'ে') ('ব', 'া') ('র', '্') ('ক', 'র') ('ক', 'া') ('ত', 'া') ('ন', 'া') ('্', 'ত') ('ন', 'ি') ('ল', 'ে') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Frequency Character\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mBERT for Max Length of 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list_2k = []\n",
    "max_len = 2000\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        token_list_2k.append(mBERT_tokenize_text(line, max_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "।  ##র  [CLS]  [SEP]  ##ি  ##ে  স  ব  ##া  ##ন  প  ##ু  ,  ক  ##ম  ##ো  ##্  আ  ##ের  ##ক  "
     ]
    }
   ],
   "source": [
    "# Uni-gram tokens\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_2k)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Bi-gram Tokens are: \n",
      "('।', '[SEP]') ('প', '##্র') ('ব', '##ি') ('স', '##ম') ('##ম', '##্') ('ক', '##র') ('##ন', ',') ('স', '##ং') ('##্', '##প') ('ন', '##ির') ('পর', '##ি') ('দ', '##ে') ('##চ', '##্') ('প', '##া') ('ম', '##ু') ('##্', '##ষ') ('##া', '##ঁ') ('##ন', '।') ('জ', '##ানা') ('.', '.') "
     ]
    }
   ],
   "source": [
    "# Bi-gram tokens\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_2k)\n",
    "print('The Top-20 Bi-gram Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_2k )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Bi-gram Frequency Syllable:\n",
      "('য', 'ে') ('য', 'া') ('প', 'র') ('ক', 'র') ('র', 'স') ('র', 'প') ('র', 'ক') ('ত', 'র') ('র', 'ব') ('দে', 'র') ('ন', 'ত') ('ন', 'র') ('স', 'ম') ('ন', '।') ('র', 'ম') ('ে', 'ছে') ('ও', 'য') ('এ', 'ক') ('দ', 'র') ('র', 'র') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Syllables\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Bi-gram Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corpus = form_char_corpus_from_token(token_list_2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('য', '়') ('া', 'র') ('ে', 'র') ('্', 'র') ('া', 'ন') ('্', 'য') ('র', 'া') ('র', 'ে') ('ন', '্') ('ে', 'ন') ('়', 'ে') ('ব', 'া') ('র', '্') ('ক', 'র') ('ক', 'া') ('ত', 'া') ('ন', 'া') ('্', 'ত') ('ন', 'ি') ('ল', 'ে') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Char\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the earlier given corpus by mBERT Tokenizer\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "max_len = 1000\n",
    "token_list_1k = []\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    for i in range(n_lines):\n",
    "            line = f.readline()\n",
    "            if i % 3 == 0: \n",
    "                token_list_1k.append(mBERT_tokenize_text(line, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '1', '.', 'কিন্তু', 'সে', '##টার', 'গ', '##ায়', '##ে', 'হ', '##াত', 'দিয়ে', 'তিনি', 'আ', '##ঁ', '##ত', '##কে', 'ও', '##ঠে', '##ন', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision, recall and F1 Score\n",
    "def metric_per_line(output_token, ground_truth_token):\n",
    "    output_token = set(output_token)\n",
    "    ground_truth_token = set(ground_truth_token)\n",
    "    true_positive = len( output_token & ground_truth_token)\n",
    "    false_positive = len( output_token - ground_truth_token)\n",
    "    false_negative = len(ground_truth_token - output_token)\n",
    "\n",
    "    if (true_positive + false_positive) > 0:\n",
    "        precision = true_positive / ( true_positive + false_positive )\n",
    "    else:\n",
    "        precision = 0\n",
    "    if ( true_positive + false_negative) > 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1_score = 2 / ((1/precision) + (1 / recall))\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    return (precision, recall, f1_score)\n",
    "\n",
    "def mean_metric_calculate(output_tokens, ground_truth_txt_filename = 'cs689_assignment.txt'):\n",
    "\n",
    "    n_lines = len(output_tokens) # Num of Lines\n",
    "    (sum_precision, sum_recall, sum_f1_score) = (0, 0, 0)\n",
    "    with open(ground_truth_txt_filename, 'r') as f:\n",
    "        for i in range(n_lines):\n",
    "            temp = f.readline() # Removing the text line\n",
    "            ground_truth = f.readline()  # Reading the even number sentences of the labels\n",
    "            temp = f.readline()\n",
    "            (t_p, t_r, t_f) = metric_per_line(output_tokens[i], ground_truth)\n",
    "            sum_precision += t_p\n",
    "            sum_recall += t_r\n",
    "            sum_f1_score += t_f\n",
    "    (mean_precision, mean_recall, mean_f1_score) = (sum_precision / n_lines, sum_recall / n_lines, sum_f1_score / n_lines)\n",
    "\n",
    "    return (mean_precision, mean_recall, mean_f1_score)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preision, recall, f1_score) = mean_metric_calculate(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.16535584613130255, Recall: 0.19240626222627186, F1 Score: 0.17692844282523296\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {preision}, Recall: {recall}, F1 Score: {f1_score}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
