{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dfb9f0-397c-483b-98b3-03299ab97507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2ce6af7-7321-405d-84ae-485cb20cffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_o_consonants = {\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'ঙ',\n",
    "    'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ',\n",
    "    'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম',\n",
    "    'য', 'য়', 'র', 'ল',\n",
    "    'শ', 'ষ', 'স', 'হ',\n",
    "    'ক়', 'খ়', 'গ়', 'জ়', 'ফ়'\n",
    "}\n",
    "# set_o_punctuations = {\n",
    "#     '!', '।', '\\n', ',', '-', '(', ')', '?', '.'\n",
    "# }\n",
    "set_o_vowels = {\n",
    "    'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ',\n",
    "    'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'\n",
    "}\n",
    "set_o_special_tokens = {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "set_o_independent_vowels = {'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ'}\n",
    "set_o_dependent_vowels = { 'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d172a1-3bfe-4b12-8eb9-d06fee17047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_space_tokenizer(text):\n",
    "    return text.split(' ') #Splitting Based Upon White Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cadea3b-dc49-438e-99f9-d216162e3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "    \n",
    "def find_unigram_freq(corpus):\n",
    "    # Find Uni-gram Frequency of the elements of the corpus\n",
    "    uni_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != ' ':\n",
    "                if line[j] in uni_dict.keys():\n",
    "                    uni_dict[line[j]] = uni_dict[line[j]] + 1\n",
    "                else:\n",
    "                    uni_dict[line[j]] = 0\n",
    "\n",
    "    keys_list = list(uni_dict.keys())\n",
    "    values_list = list(uni_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)\n",
    "    \n",
    "def form_bigram_freq(corpus):\n",
    "    # Find Bi-gram Frequency of the elements of the corpus\n",
    "    bigram_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line) - 1):\n",
    "            if line[j] != ' ' and line[j+1] != ' ' and line[j] != '#':\n",
    "                if (line[j], line[j+1]) in bigram_dict.keys():\n",
    "                    bigram_dict[(line[j], line[j+1])] = bigram_dict[(line[j], line[j+1])] + 1\n",
    "                else:\n",
    "                    bigram_dict[(line[j], line[j+1])] = 0\n",
    "\n",
    "    keys_list = list(bigram_dict.keys())\n",
    "    values_list = list(bigram_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)\n",
    "\n",
    "def syllable_form_from_line(text):\n",
    "\n",
    "    #Split the text into Syllable\n",
    "    \n",
    "    syllable_list = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "\n",
    "        n = len(text[i])\n",
    "        if n == 1:\n",
    "            syllable_list.append(text[i])\n",
    "            i = i+ 1\n",
    "        else:\n",
    "            j = 0\n",
    "            while j<len(text[i]):\n",
    "                syllable = ''\n",
    "                while text[i][j] == '#' and (j+1)<len(text[i]):\n",
    "                    j = j + 1\n",
    "                if text[i][j] in set_o_independent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                elif text[i][j] in set_o_consonants:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    if (j+1) < len(text[i]) :\n",
    "                        if text[i][j+1] in set_o_consonants or text[i][j+1] == 'অ':\n",
    "                            syllable = syllable + text[i][j+1]\n",
    "                            j = j + 1\n",
    "                j = j + 1\n",
    "                while j < len(text[i]) and text[i][j] in set_o_dependent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    j = j + 1\n",
    "                if syllable != '':\n",
    "                    syllable_list.append(syllable)\n",
    "            i = i + 1\n",
    "\n",
    "    return syllable_list\n",
    "\n",
    "def syllable_form_from_whole_corpus(unicode_corrected_corpus):\n",
    "    \"\"\"\n",
    "    Args: A List of List of Unicode corrected Chars corresponding to each sentence, words are separated by ' '\n",
    "    \"\"\"\n",
    "    syllable_corpus = []\n",
    "    for i in range(len(unicode_corrected_corpus)):\n",
    "        syllable_corpus.append(syllable_form_from_line(unicode_corrected_corpus[i]))\n",
    "    return syllable_corpus\n",
    "    \n",
    "def form_char_corpus_from_token(token_list_1k):\n",
    "\n",
    "    char_corpus = []\n",
    "    for i in range(len(token_list_1k)):\n",
    "        char_line_corpus = []\n",
    "        for j in range(len(token_list_1k[i])):\n",
    "            if token_list_1k[i][j] not in set_o_special_tokens:\n",
    "                text = token_list_1k[i][j].replace('▁', '')\n",
    "                char_line_corpus += list(text)\n",
    "        char_corpus.append(char_line_corpus)\n",
    "\n",
    "    return char_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6d2a19-6a50-475c-a1a3-c05f50103bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "ও  করে  এই  থেকে  এ  এবং  করা  তার  না  এক  তিনি  জন্য  আর  নিয়ে  করতে  একটি  সঙ্গে  যে  কিন্তু  বলেন,  "
     ]
    }
   ],
   "source": [
    "token_list_corpus = []\n",
    "max_len = 1000\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        token_list_corpus.append(white_space_tokenizer(line))\n",
    "# Uni-gram Tokens\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9094e8b1-1a67-47a3-86a2-aaf1a85119b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "('', '')  ('.', '.')  ('তিনি', 'বলেন,')  ('করা', 'হয়।\\n')  ('করা', 'হয়েছে।\\n')  ('সাধারণ', 'সম্পাদক')  ('আওয়ামী', 'লীগের')  ('এ', 'সময়')  ('শেষ', 'আপডেট:')  ('করা', 'হয়।')  ('করা', 'হয়েছে।')  ('এর', 'আগে')  ('করার', 'জন্য')  ('কাছ', 'থেকে')  ('করা', 'হয়েছে')  ('প্রধানমন্ত্রী', 'শেখ')  ('পক্ষ', 'থেকে')  ('শুরু', 'করে')  ('আওয়ামী', 'লীগ')  ('এ', 'বিষয়ে')  "
     ]
    }
   ],
   "source": [
    "# Bi-Gram Tokens\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cdd166d-df0a-41da-88b1-a239e6f9fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc024a47-3ff0-46cb-89bd-23f7c883141b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Bi-gram Frequency Syllable:\n",
      "('দে', 'র') ('ে', 'ছে') ('ও', 'া') ('হ', 'ে') ('ছে', 'ন') ('রে', 'র') ('থে', 'কে') ('র', 'আ') ('এ', 'ই') ('নে', 'র') ('কা', 'র') ('বা', 'র') ('আ', 'র') ('র', 'এ') ('তা', 'র') ('া', 'র') ('র', 'প') ('র', 'অ') ('নি', 'ে') ('র', 'বি') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Syllables\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Bi-gram Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2db2e98-a4a0-4bcc-a46d-75df0f9c0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corpus = form_char_corpus_from_token(token_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5bac0fa-1d66-4d19-9101-b4dc71749be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('া', 'র') ('ে', 'র') ('্', 'র') ('া', 'ন') ('্', 'য') ('র', 'া') ('র', 'ে') ('ন', '্') ('ে', 'ন') ('ব', 'া') ('।', '\\n') ('র', '্') ('ক', 'র') ('ক', 'া') ('ত', 'া') ('ন', 'া') ('্', 'ত') ('ল', 'ে') ('ন', 'ি') ('ক', 'ে') "
     ]
    }
   ],
   "source": [
    "# Bi-gram Characters\n",
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e8c92-39b5-4259-8d76-8a8168d289ba",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760473cd-9163-4429-a177-caf18d6550b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the earlier given corpus by mBERT Tokenizer\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "max_len = 1000\n",
    "token_list_1k = []\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        if i % 3 == 0:\n",
    "            token_list_1k.append(white_space_tokenizer( line ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7966bc64-c830-4099-96df-7073744f5066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.', 'স্থির', 'হয়,', 'কোনও', 'ক্ষেত্রে', 'মতবিরোধ', 'থাকলে', 'উভয়পক্ষই', 'সংযম', 'দেখাবে।\\n']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_1k[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e5f3cfd-2255-46f8-9167-413f5725f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision, recall and F1 Score\n",
    "def metric_per_line(output_token, ground_truth_token):\n",
    "    output_token = set(output_token)\n",
    "    ground_truth_token = set(ground_truth_token)\n",
    "    true_positive = len( output_token & ground_truth_token)\n",
    "    false_positive = len( output_token - ground_truth_token)\n",
    "    false_negative = len(ground_truth_token - output_token)\n",
    "\n",
    "    if (true_positive + false_positive) > 0:\n",
    "        precision = true_positive / ( true_positive + false_positive )\n",
    "    else:\n",
    "        precision = 0\n",
    "    if ( true_positive + false_negative) > 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1_score = 2 / ((1/precision) + (1 / recall))\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    return (precision, recall, f1_score)\n",
    "\n",
    "def mean_metric_calculate(output_tokens, ground_truth_txt_filename = 'cs689_assignment.txt'):\n",
    "\n",
    "    n_lines = len(output_tokens) # Num of Lines\n",
    "    (sum_precision, sum_recall, sum_f1_score) = (0, 0, 0)\n",
    "    with open(ground_truth_txt_filename, 'r') as f:\n",
    "        for i in range(n_lines):\n",
    "            temp = f.readline() # Removing the text line\n",
    "            ground_truth = f.readline()  # Reading the even number sentences of the labels\n",
    "            temp = f.readline() # Removing Exra-ine\n",
    "            # print(ground_truth)\n",
    "            # print(type\n",
    "            (t_p, t_r, t_f) = metric_per_line(output_tokens[i], ground_truth)\n",
    "            sum_precision += t_p\n",
    "            sum_recall += t_r\n",
    "            sum_f1_score += t_f\n",
    "    (mean_precision, mean_recall, mean_f1_score) = (sum_precision / n_lines, sum_recall / n_lines, sum_f1_score / n_lines)\n",
    "\n",
    "    return (mean_precision, mean_recall, mean_f1_score)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26379bb3-fb85-4ccf-bf03-7a71dd0cfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "(preision, recall, f1_score) = mean_metric_calculate(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd6e6be7-edb3-4643-9a0d-de7f72d57357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.032878510378510376, Recall: 0.015375640161614917, F1 Score: 0.0208486781247428\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {preision}, Recall: {recall}, F1 Score: {f1_score}' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
