{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cb9ffd-9955-4fa5-a7da-4a9056e3498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c84856ea-848d-4b92-8f27-b8d2e822688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_o_consonants = {\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'ঙ',\n",
    "    'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ',\n",
    "    'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম',\n",
    "    'য', 'য়', 'র', 'ল',\n",
    "    'শ', 'ষ', 'স', 'হ',\n",
    "    'ক়', 'খ়', 'গ়', 'জ়', 'ফ়'\n",
    "}\n",
    "# set_o_punctuations = {\n",
    "#     '!', '।', '\\n', ',', '-', '(', ')', '?', '.'\n",
    "# }\n",
    "set_o_vowels = {\n",
    "    'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ',\n",
    "    'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'\n",
    "}\n",
    "# set_o_special_tokens = {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "set_o_independent_vowels = {'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'ৠ', 'ঌ', 'ৡ', 'এ', 'ঐ', 'ও', 'ঔ'}\n",
    "set_o_dependent_vowels = { 'ং', 'ঁ', 'ঃ','ৎ', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', ' ো', ' ৌ', 'া', 'ি', 'ী'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a94e0a-acb1-4599-9f07-e434ab890840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: bn_100.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: bn_100.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (4220 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 339034 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 445 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39162278\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.951% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=155\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99951\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 339034 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=19020999\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 595876 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 339034\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 472987\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 472987 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=194264 obj=13.1382 num_tokens=1028956 num_tokens/piece=5.29669\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=170423 obj=10.7821 num_tokens=1031347 num_tokens/piece=6.05169\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=127764 obj=10.7661 num_tokens=1073662 num_tokens/piece=8.40348\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=127617 obj=10.7503 num_tokens=1074326 num_tokens/piece=8.41836\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=95708 obj=10.8141 num_tokens=1128690 num_tokens/piece=11.7931\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=95699 obj=10.7993 num_tokens=1129742 num_tokens/piece=11.8052\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=71774 obj=10.8874 num_tokens=1187707 num_tokens/piece=16.5479\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=71773 obj=10.8696 num_tokens=1187703 num_tokens/piece=16.548\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=53829 obj=10.9841 num_tokens=1248831 num_tokens/piece=23.2\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=53829 obj=10.9607 num_tokens=1248843 num_token"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s/piece=23.2002\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=40371 obj=11.1044 num_tokens=1313330 num_tokens/piece=32.5315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=40371 obj=11.0753 num_tokens=1313365 num_tokens/piece=32.5324\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=30278 obj=11.2589 num_tokens=1379818 num_tokens/piece=45.5716\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=30278 obj=11.2222 num_tokens=1379876 num_tokens/piece=45.5736\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=22708 obj=11.4423 num_tokens=1449533 num_tokens/piece=63.8336\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=22708 obj=11.3971 num_tokens=1449629 num_tokens/piece=63.8378\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17031 obj=11.6638 num_tokens=1523530 num_tokens/piece=89.4563\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17031 obj=11.6077 num_tokens=1523678 num_tokens/piece=89.465\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12773 obj=11.9195 num_tokens=1600235 num_tokens/piece=125.283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12773 obj=11.854 num_tokens=1600354 num_tokens/piece=125.292\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9579 obj=12.2119 num_tokens=1682164 num_tokens/piece=175.61\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9579 obj=12.1329 num_tokens=1682280 num_tokens/piece=175.622\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7184 obj=12.5386 num_tokens=1769250 num_tokens/piece=246.276\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7184 obj=12.4491 num_tokens=1769433 num_tokens/piece=246.302\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5388 obj=12.9049 num_tokens=1864346 num_tokens/piece=346.018\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5388 obj=12.805 num_tokens=1864482 num_tokens/piece=346.043\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4041 obj=13.3303 num_tokens=1962721 num_tokens/piece=485.702\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4041 obj=13.2137 num_tokens=1962817 num_tokens/piece=485.726\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3030 obj=13.7876 num_tokens=2068186 num_tokens/piece=682.57\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3030 obj=13.6592 num_tokens=2068294 num_tokens/piece=682.605\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2272 obj=14.2931 num_tokens=2181449 num_tokens/piece=960.145\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2272 obj=14.1486 num_tokens=2181505 num_tokens/piece=960.169\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=14.2133 num_tokens=2196556 num_tokens/piece=998.435\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=14.1966 num_tokens=2196685 num_tokens/piece=998.493\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: unigram_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: unigram_model.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(input='bn_100.txt', model_prefix= \"unigram_model\", vocab_size=2000)\n",
    "sp = spm.SentencePieceProcessor('unigram_model.model')\n",
    "sp.load('unigram_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02f94a6b-f930-487a-b4f7-3bd819f850ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "    \n",
    "def find_unigram_freq(corpus):\n",
    "    uni_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line)):\n",
    "            if line[j] != ' ':\n",
    "                if line[j] in uni_dict.keys():\n",
    "                    uni_dict[line[j]] = uni_dict[line[j]] + 1\n",
    "                else:\n",
    "                    uni_dict[line[j]] = 0\n",
    "\n",
    "    keys_list = list(uni_dict.keys())\n",
    "    values_list = list(uni_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)\n",
    "    \n",
    "def form_bigram_freq(corpus):\n",
    "    bigram_dict = {}\n",
    "    for i in range(len(corpus)):\n",
    "        line = corpus[i]\n",
    "        for j in range(len(line) - 1):\n",
    "            if line[j] != ' ' and line[j+1] != ' ' and line[j] != '#':\n",
    "                if (line[j], line[j+1]) in bigram_dict.keys():\n",
    "                    bigram_dict[(line[j], line[j+1])] = bigram_dict[(line[j], line[j+1])] + 1\n",
    "                else:\n",
    "                    bigram_dict[(line[j], line[j+1])] = 0\n",
    "\n",
    "    keys_list = list(bigram_dict.keys())\n",
    "    values_list = list(bigram_dict.values())\n",
    "    sorted_value_index = np.argsort(values_list)\n",
    "    sorted_char_dict = {keys_list[i]: values_list[i] for i in sorted_value_index}\n",
    "\n",
    "    return (keys_list, sorted_value_index)\n",
    "\n",
    "def syllable_form_from_line(text):\n",
    "\n",
    "    syllable_list = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "\n",
    "        n = len(text[i])\n",
    "        if n == 1:\n",
    "            syllable_list.append(text[i])\n",
    "            i = i+ 1\n",
    "        else:\n",
    "            j = 0\n",
    "            while j<len(text[i]):\n",
    "                syllable = ''\n",
    "                while text[i][j] == '#' and (j+1)<len(text[i]):\n",
    "                    j = j + 1\n",
    "                if text[i][j] in set_o_independent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                elif text[i][j] in set_o_consonants:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    if (j+1) < len(text[i]) :\n",
    "                        if text[i][j+1] in set_o_consonants or text[i][j+1] == 'অ':\n",
    "                            syllable = syllable + text[i][j+1]\n",
    "                            j = j + 1\n",
    "                j = j + 1\n",
    "                while j < len(text[i]) and text[i][j] in set_o_dependent_vowels:\n",
    "                    syllable = syllable + text[i][j]\n",
    "                    j = j + 1\n",
    "                if syllable != '':\n",
    "                    syllable_list.append(syllable)\n",
    "            i = i + 1\n",
    "\n",
    "    return syllable_list\n",
    "\n",
    "def syllable_form_from_whole_corpus(unicode_corrected_corpus):\n",
    "    \"\"\"\n",
    "    Args: A List of List of Unicode corrected Chars corresponding to each sentence, words are separated by ' '\n",
    "    \"\"\"\n",
    "    syllable_corpus = []\n",
    "    for i in range(len(unicode_corrected_corpus)):\n",
    "        syllable_corpus.append(syllable_form_from_line(unicode_corrected_corpus[i]))\n",
    "    return syllable_corpus\n",
    "    \n",
    "def form_char_corpus_from_token(token_list_1k):\n",
    "\n",
    "    char_corpus = []\n",
    "    for i in range(len(token_list_1k)):\n",
    "        char_line_corpus = []\n",
    "        for j in range(len(token_list_1k[i])):\n",
    "            # if token_list_1k[i][j] not in set_o_special_tokens:\n",
    "            text = token_list_1k[i][j].replace('▁', '')\n",
    "            char_line_corpus += list(text)\n",
    "        char_corpus.append(char_line_corpus)\n",
    "\n",
    "    return char_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9d3dd98-6800-4f22-ac46-0a168ffbdf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "।  র  ে  ,  ের  ন  ▁  ই  ল  ক  ম  য়  কে  স  ত  তে  ী  রা  ▁আ  ি  "
     ]
    }
   ],
   "source": [
    "token_list_corpus = []\n",
    "# max_len = 2000\n",
    "with open('bn_100.txt', \"r\", errors = 'ignore') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        # print(line)\n",
    "        token_list_corpus.append(sp.encode_as_pieces(line))\n",
    "( keys_list, sorted_value_index ) = find_unigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "050a1903-0037-4cb4-b9c7-9fc7864335be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Top-20 Frequent Tokens are: \n",
      "('▁না', '।')  ('ন', '।')  ('▁হয়', '।')  ('▁বলেন', ',')  ('▁হয়েছে', '।')  ('▁হবে', '।')  ('র', 'ের')  (',', '▁')  ('ন', ',')  ('ে', '।')  ('কে', 'র')  ('ছে', '।')  (',', '▁‘')  ('।', '▁এ')  ('▁করেন', '।')  ('রা', '।')  ('দ', 'ে')  ('ের', '▁জন্য')  ('▁করা', '▁হয়')  ('নি', '।')  "
     ]
    }
   ],
   "source": [
    "( keys_list, sorted_value_index ) = form_bigram_freq(token_list_corpus)\n",
    "print('The Top-20 Frequent Tokens are: ')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = '  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c70887da-cf38-45aa-bf19-e668642c2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_corpus = syllable_form_from_whole_corpus( token_list_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99745ca2-a27f-4f3e-9885-51b4b30b34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Bi-gram Frequency Syllable:\n",
      "('য', 'ে') ('য', 'া') ('দে', 'র') ('প', 'র') ('ে', 'ছে') ('ন', '।') ('ও', 'য') ('ত', 'র') ('ন', ',') ('র', 'র') ('কা', 'র') ('ছে', '।') ('হয', 'ে') ('বা', 'র') ('নি', 'য') ('ছে', 'ন') ('ন', 'ত') ('এ', 'ক') ('থে', 'কে') ('তা', 'র') "
     ]
    }
   ],
   "source": [
    "( keys_list, sorted_value_index ) = form_bigram_freq(syllable_corpus)\n",
    "print('The top 20 Bi-gram Frequency Syllable:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "222785df-85e9-4140-89e2-c351be2b465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_corpus = form_char_corpus_from_token(token_list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3e56ce6-a886-4e4a-b841-cda31da8f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 20 Frequent Characters:\n",
      "('য', '়') ('া', 'র') ('ে', 'র') ('্', 'র') ('া', 'ন') ('্', 'য') ('র', 'া') ('র', 'ে') ('ন', '্') ('ে', 'ন') ('়', 'ে') ('ব', 'া') ('র', '্') ('ক', 'র') ('ক', 'া') ('ত', 'া') ('ন', 'া') ('্', 'ত') ('ন', 'ি') ('ল', 'ে') "
     ]
    }
   ],
   "source": [
    "( keys_list, sorted_value_index ) = form_bigram_freq(char_corpus)\n",
    "print('The top 20 Frequent Characters:')\n",
    "for i in sorted_value_index[::-1][:20]:\n",
    "    print(keys_list[i], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc251f-d733-49a7-b48e-25445a8bbe44",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42eeb0f0-5cbe-490d-94f4-cfd53fbcd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the earlier given corpus by mBERT Tokenizer\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    n_lines = len(f.readlines())\n",
    "max_len = 1000\n",
    "token_list_1k = []\n",
    "with open('cs689_assignment.txt', 'r') as f:\n",
    "    for i in range(n_lines):\n",
    "        line = f.readline()\n",
    "        if i % 3 == 0:    \n",
    "            token_list_1k.append(sp.encode_as_pieces(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a557bb81-b8b9-4631-ab26-fe65675b80ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '1', '.', '▁কিন্তু', '▁সেটা', 'র', '▁গা', 'য়ে', '▁হাত', '▁দিয়ে', '▁তিনি', '▁আ', 'ঁ', 'ত', 'কে', '▁ওঠে', 'ন', '।']\n"
     ]
    }
   ],
   "source": [
    "print(token_list_1k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c874f4-4eef-4daa-b711-8c854db80bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the precision, recall and F1 Score\n",
    "def metric_per_line(output_token, ground_truth_token):\n",
    "    output_token = set(output_token)\n",
    "    ground_truth_token = set(ground_truth_token)\n",
    "    true_positive = len( output_token & ground_truth_token)\n",
    "    false_positive = len( output_token - ground_truth_token)\n",
    "    false_negative = len(ground_truth_token - output_token)\n",
    "\n",
    "    if (true_positive + false_positive) > 0:\n",
    "        precision = true_positive / ( true_positive + false_positive )\n",
    "    else:\n",
    "        precision = 0\n",
    "    if ( true_positive + false_negative) > 0:\n",
    "        recall = true_positive / (true_positive + false_negative)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if precision > 0 and recall > 0:\n",
    "        f1_score = 2 / ((1/precision) + (1 / recall))\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "    return (precision, recall, f1_score)\n",
    "\n",
    "def mean_metric_calculate(output_tokens, ground_truth_txt_filename = 'cs689_assignment.txt'):\n",
    "\n",
    "    n_lines = len(output_tokens) # Num of Lines\n",
    "    (sum_precision, sum_recall, sum_f1_score) = (0, 0, 0)\n",
    "    with open(ground_truth_txt_filename, 'r') as f:\n",
    "        for i in range(n_lines):\n",
    "            temp = f.readline() # Removing the text line\n",
    "            ground_truth = f.readline()  # Reading the even number sentences of the labels\n",
    "            temp = f.readline()\n",
    "            (t_p, t_r, t_f) = metric_per_line(output_tokens[i], ground_truth)\n",
    "            sum_precision += t_p\n",
    "            sum_recall += t_r\n",
    "            sum_f1_score += t_f\n",
    "    (mean_precision, mean_recall, mean_f1_score) = (sum_precision / n_lines, sum_recall / n_lines, sum_f1_score / n_lines)\n",
    "\n",
    "    return (mean_precision, mean_recall, mean_f1_score)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320d6b85-ff83-457d-aae5-9aa03892cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(preision, recall, f1_score) = mean_metric_calculate(token_list_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c63768-4d7e-4269-9ca3-9ff42e4c2dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.16980305454521108, Recall: 0.1554242088211719, F1 Score: 0.1610034428684715\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {preision}, Recall: {recall}, F1 Score: {f1_score}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6625d6-a317-49bb-922d-8acc512be3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
